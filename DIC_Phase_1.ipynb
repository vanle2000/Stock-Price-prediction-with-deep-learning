{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanle2000/Stock-Price-prediction-with-deep-learning/blob/main/DIC_Phase_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B375St7tAfJl",
        "outputId": "9ba77395-c7b6-4eb1-b13d-cb16aff61070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' File creation Date: 14.09.2023 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "''' File creation Date: 14.09.2023 '''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Introduction:\n",
        "- problem statement\n",
        "- potential contribution: why this contribution is crucial?\n",
        "\n",
        "This project we aim to build a recommendation system for stocks buying in the 3rd quarter that used historical stocks prices, modalities, and news (if possible) from 1st and 2nd quarter of 2023 in order to recommend relevant stocks for user interests.\n",
        "\n"
      ],
      "metadata": {
        "id": "xVwoTNrcGuiB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ck3p9KmzX3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muWawHxMzWl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset:"
      ],
      "metadata": {
        "id": "rTNEhzFzHVbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Import library:"
      ],
      "metadata": {
        "id": "616WelX0HiN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark[pandas_on_spark] plotly"
      ],
      "metadata": {
        "id": "4Z6baVSBUNoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551892e1-678a-4e09-829e-486ed0cb66c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark[pandas_on_spark]\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]) (1.23.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pyspark[pandas_on_spark]) (1.16.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=f20179cff07580e5b20fcec2d0c8b095b79b24fc0b6fb3b87c28c760d8536066\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import requests\n",
        "import glob\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime, date\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "# from alpha_vantage.techindicators import TechIndicators\n",
        "# from alpha_vantage.sectorperformance import SectorPerformances\n",
        "# from alpha_vantage.cryptocurrencies import CryptoCurrencies\n",
        "# from alpha_vantage.foreignexchange import ForeignExchange"
      ],
      "metadata": {
        "id": "6m8B_voGHJd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afb32fc-5dd1-4958-a08d-8810f8d654e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2a4837e04ee4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malpha_vantage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Data Acquisitions:\n",
        "\n",
        "* Historical Stock Price Data: This dataset contains historical stock price information, including open, close, high, low prices, and trading volumes. We retrieve real time, historical and technical indicators financial data providers including Yahoo Finance, Alpha Vantage API.\n",
        "\n",
        "* Fundamental Data: Fundamental data includes financial metrics such as earnings per share (EPS), price-to-earnings ratio (P/E), market capitalization, debt ratios, and more. You can often find this data in financial reports, financial news sources, or specialized financial data providers.\n",
        "\n",
        "* Macroeconomic Indicators: Economic indicators like GDP growth, inflation rates, interest rates, and unemployment rates can impact the overall market and individual stocks. You can obtain this data from government sources or economic data providers.\n",
        "\n",
        "* Sector and Industry Information: Stocks within the same sector or industry often move together. Data on sectors and industries can be obtained from financial news sources or sector-specific databases.\n",
        "\n",
        "* Market Index Data: Data on major market indices like the S&P 500, NASDAQ, or Dow Jones can be useful for benchmarking and analyzing stock performance relative to broader market trends.\n",
        "\n",
        "* Earnings Call Transcripts: Analyzing transcripts of earnings calls can provide insights into a company's management outlook and future plans.\n",
        "\n",
        "* News and Social Media Data (if time permits): News articles, social media sentiment, and market news can influence stock prices. Accessing APIs or scraping news articles and social media platforms can provide valuable sentiment data.\n",
        "\n",
        "* Machine Learning Features: You may generate additional features through feature engineering or sentiment analysis on news and social media data to feed into your recommendation algorithm."
      ],
      "metadata": {
        "id": "Ew-JSlDUHmkV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_iaR-6FkxJhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2.1. Core Stocks and Financial Indicators Retrieval:"
      ],
      "metadata": {
        "id": "RoQh_ak_0WoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Request the data from API key\n",
        "url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=META&interval=60min&apikey=MNYH22VAN1CPKDLG'\n",
        "r = requests.get(url)\n",
        "data = r.json()\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "a3yNnQ0QSQPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ce2e5b-c3c2-4df2-9294-b31064054a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2d75226d0ae4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Request the data from API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=META&interval=60min&apikey=MNYH22VAN1CPKDLG'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the API key\n",
        "api_key = \"MNYH22VAN1CPKDLG\"\n",
        "\n",
        "# Read ticker symbols from a file to python list object\n",
        "symbols = []\n",
        "with open('/content/nasdaq_ticker_symbols.csv') as f:\n",
        "    for line in f:\n",
        "        symbols.append(line.strip())\n",
        "f.close\n",
        "\n",
        "\n",
        "def get_alpha_vantage_data(api_key, symbols, function):\n",
        "    # Define the API endpoint\n",
        "    endpoint = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "    # Create a list to hold the data frames for each symbol\n",
        "    dfs = []\n",
        "\n",
        "    for symbol in symbols:\n",
        "        # Construct the API request URL\n",
        "        url = f\"{endpoint}?function={function}&symbol={symbol}&apikey={api_key}\"\n",
        "\n",
        "        # Send the API request\n",
        "        r = requests.get(url)\n",
        "\n",
        "        # Parse the JSON response\n",
        "        data = r.json()\n",
        "\n",
        "        # Check if the API request was successful\n",
        "        if \"Error Message\" in data:\n",
        "            print(f\"Error retrieving data for {symbol}: {data['Error Message']}\")\n",
        "            continue\n",
        "\n",
        "        # Extract the data based on the function\n",
        "        if function.startswith(\"TIME_SERIES\"):\n",
        "            # Time series data for stocks\n",
        "            time_series_key = f\"Time Series ({function.split('_')[-1].capitalize()})\"\n",
        "            time_series = data[time_series_key]\n",
        "\n",
        "            # Create a list of dictionaries to hold the data points\n",
        "            rows = []\n",
        "\n",
        "            # Iterate over the time series data and extract specific data points\n",
        "            for date, values in time_series.items():\n",
        "                open_price = float(values[\"1. open\"])\n",
        "                high_price = float(values[\"2. high\"])\n",
        "                low_price = float(values[\"3. low\"])\n",
        "                close_price = float(values[\"4. close\"])\n",
        "\n",
        "                # Create a dictionary for each row of data\n",
        "                row = {\n",
        "                    \"Symbol\": symbol,\n",
        "                    \"Date\": date,\n",
        "                    \"Open\": open_price,\n",
        "                    \"High\": high_price,\n",
        "                    \"Low\": low_price,\n",
        "                    \"Close\": close_price\n",
        "                }\n",
        "\n",
        "                # Append the row dictionary to the list\n",
        "                rows.append(row)\n",
        "\n",
        "        elif function == \"GLOBAL_QUOTE\":\n",
        "            # Market indicators\n",
        "            global_quote = data[\"Global Quote\"]\n",
        "\n",
        "            # Create a list of dictionaries to hold the data points\n",
        "            rows = [{\n",
        "                \"Symbol\": symbol,\n",
        "                \"Open\": float(global_quote[\"02. open\"]),\n",
        "                \"High\": float(global_quote[\"03. high\"]),\n",
        "                \"Low\": float(global_quote[\"04. low\"]),\n",
        "                \"Close\": float(global_quote[\"05. price\"])\n",
        "            }]\n",
        "\n",
        "        elif function == \"ECONOMIC_DATA\":\n",
        "            # Economic indicators\n",
        "            rows = [{\n",
        "                \"Symbol\": symbol,\n",
        "                \"Function\": function\n",
        "            }]\n",
        "            for key, value in data.items():\n",
        "                if key != \"Symbol\":\n",
        "                    rows[0][key] = value\n",
        "\n",
        "        elif function == \"OVERVIEW\":\n",
        "            # Fundamental data\n",
        "            rows = [{\n",
        "                \"Symbol\": symbol,\n",
        "                \"Function\": function\n",
        "            }]\n",
        "            for key, value in data.items():\n",
        "                if key != \"Symbol\":\n",
        "                    rows[0][key] = value\n",
        "\n",
        "        else:\n",
        "            print(f\"Unsupported function: {function}\")\n",
        "            continue\n",
        "\n",
        "        # Create a SparkSession\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "        # Create a DataFrame from the list of dictionaries\n",
        "        df = spark.createDataFrame(rows)\n",
        "\n",
        "        # Add the DataFrame to the list\n",
        "        dfs.append(df)\n",
        "\n",
        "    # Union all the DataFrames into a single DataFrame\n",
        "    combined_df = dfs[0]\n",
        "    for df in dfs[1:]:\n",
        "        combined_df = combined_df.union(df)\n",
        "\n",
        "    # Return the combined DataFrame\n",
        "    return combined_df"
      ],
      "metadata": {
        "id": "3q6XrxPtSQRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4mnT0loSQWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vKgrzSz0XldL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Market News and Sentiment:"
      ],
      "metadata": {
        "id": "IteoUnU_bhRS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4fwh5MA0bitH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Exporatory Data Analysis:"
      ],
      "metadata": {
        "id": "X8t6tmqiHUZC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0iqQa5RxHfQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}